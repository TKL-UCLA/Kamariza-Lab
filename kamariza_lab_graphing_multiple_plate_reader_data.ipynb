{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports##\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read data##\n",
    "\n",
    "#step 8#\n",
    "file_path_layout_0 = \"/Users/zach/Documents/Education/Undergrad (UWEC)/Summer 2023/UCLA/optimization_protocol/06-07-2023-3HC-standard-protocol/Pre_label_od_layout.csv\"\n",
    "file_path_data_0 = \"/Users/zach/Documents/Education/Undergrad (UWEC)/Summer 2023/UCLA/optimization_protocol/06-07-2023-3HC-standard-protocol/Pre-label-od.csv\"\n",
    "\n",
    "file_path_layout_1 = \"/Users/zach/Documents/Education/Undergrad (UWEC)/Summer 2023/UCLA/optimization_protocol/06-07-2023-3HC-standard-protocol/post-label-od-flo-layout.csv\"\n",
    "file_path_data_1 = \"/Users/zach/Documents/Education/Undergrad (UWEC)/Summer 2023/UCLA/optimization_protocol/06-07-2023-3HC-standard-protocol/labeled-od.csv\"\n",
    "\n",
    "\n",
    "\n",
    "##if file .txt and not .csv then include an r before the \"\"parentheses\"\"\n",
    "##thus it will be \n",
    "##file_path_data = r\"##please input the file path or the file name##\"\n",
    "\n",
    "##copy and paste this as many times as the files you have\n",
    "##if you have 4 files make sure there are 8 file paths, 4 for the data, 4 for the layout\n",
    "\n",
    "\n",
    "df_plate_reader_layout_1 = pd.read_csv(file_path_layout_0)\n",
    "df_plate_reader_data_1 = pd.read_csv(file_path_data_0)\n",
    "\n",
    "df_plate_reader_layout_2 = pd.read_csv(file_path_layout_1)\n",
    "df_plate_reader_data_2 = pd.read_csv(file_path_data_1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "##change the size of the dictionary depending on how many files you have imported, thus if you have 4 plate reader data, \n",
    "## your dictionary size should be 4 with a list of 2 dataframes at each index\n",
    "a_dict_of_all_data = {\n",
    "    0: [df_plate_reader_layout_0, df_plate_reader_data_0], \n",
    "    1: [df_plate_reader_layout_1, df_plate_reader_data_1], \n",
    "                      }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Is Nan function\n",
    "\n",
    "def isNaN(num):\n",
    "    return num != num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function that finds the number of groups in the layout\n",
    "def unique_samples_from_plate_layout(df):\n",
    "    df = df.iloc[:, 1:].reset_index(drop=True)\n",
    "    samples = set()\n",
    "\n",
    "    for i in range(df.shape[0] - 1):\n",
    "        for j in range(df.shape[1] - 1):\n",
    "            unique_sample = df.iloc[i, j]\n",
    "\n",
    "            if pd.isna(unique_sample):\n",
    "                continue\n",
    "\n",
    "            samples.add(unique_sample)\n",
    "\n",
    "    return list(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##finding values in the data that are mapped to the same layout position\n",
    "def find_values(string, layout, data_frame):\n",
    "    layout = layout.iloc[:, 1:]\n",
    "    data_frame = data_frame.iloc[:, 1:]\n",
    "    values = []\n",
    "\n",
    "    for ii in range(layout.shape[0]):\n",
    "        for jj in range(layout.shape[1]):\n",
    "            if layout.iloc[ii, jj] == string:\n",
    "                values.append(data_frame.iloc[ii, jj])\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function that enumerates through the groups of the layout\n",
    "##Extracts the data in the corresponding plate map\n",
    "##placing this information into the correct column\n",
    "def developing_new_df_from_input_data(layout, data_plate_reader):\n",
    "    column_titles = unique_samples_from_plate_layout(layout)\n",
    "    num_rows = layout.shape[0]\n",
    "    num_cols = layout.shape[1]\n",
    "\n",
    "    df = pd.DataFrame(index=range(num_rows), columns=column_titles)\n",
    "\n",
    "    for ii, title in enumerate(column_titles):\n",
    "        cell_position = 0\n",
    "\n",
    "        for jj in range(num_rows):\n",
    "            for kk in range(num_cols):\n",
    "                if layout.iloc[jj, kk] == title:\n",
    "                    df.iloc[cell_position, ii] = data_plate_reader.iloc[jj, kk]\n",
    "                    cell_position += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##A function to obtain the original density after a dilution\n",
    "def get_OD(ratio, final_volume, cell_position):\n",
    "    ratio_nums = ratio.split(':')\n",
    "    multiplyer = int (int(ratio_nums[1]) / int(ratio_nums[0]) )\n",
    "    a = multiplyer*final_volume*cell_position\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function to calculate the original density and put it into the dataframe\n",
    "def calculate_the_original_density(df, columns_names_ajusted, diluted_to, final_volume):\n",
    "    for ii in range(len(columns_names_ajusted)):\n",
    "\n",
    "        name = columns_names_ajusted[ii]\n",
    "\n",
    "        df[str(name)] = get_OD(str(diluted_to[ii]), float(final_volume), df[str(name)])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_statistically_significant_groups_update(list_of_columns, p_value, dataframe_df, scaling_factor):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import itertools\n",
    "    import scipy.stats as stats\n",
    "\n",
    "    # Set up the plot\n",
    "    sns.set(style='whitegrid')\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 6), dpi=600)\n",
    "    new_df = dataframe_df[list_of_columns]\n",
    "\n",
    "    # Plot the data\n",
    "    sns.boxplot(data=new_df, ax=axes)\n",
    "\n",
    "    boxplot_height = axes.get_ylim()[1]  # Get the maximum height of the box plots\n",
    "\n",
    "    line_offset = scaling_factor  # Adjust this value to control the spacing between the lines\n",
    "\n",
    "    for i in range(len(list_of_columns)):\n",
    "        for j in range(i+1, len(list_of_columns)):\n",
    "            t_stat, p_val = stats.ttest_ind(new_df.iloc[:, i], new_df.iloc[:, j])\n",
    "\n",
    "            if p_val <= p_value:\n",
    "                x_pos = (i + j) / 2\n",
    "                y_pos = boxplot_height + line_offset  # Adjust the y-coordinate to be above the previous lines\n",
    "\n",
    "                plt.text(x_pos, y_pos, f\"p={p_val:.2e}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "                plt.plot([i, j], [y_pos, y_pos], 'k--')\n",
    "\n",
    "                line_offset += scaling_factor  # Increase the line offset for the next line\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function that enumerates through the groups of the layout\n",
    "##Extracts the data in the corresponding plate map\n",
    "##placing this information into the correct column\n",
    "def developing_new_df_from_input_data(layout, data_plate_reader):\n",
    "    column_titles = unique_samples_from_plate_layout(layout)\n",
    "    num_rows = layout.shape[0]\n",
    "    num_cols = layout.shape[1]\n",
    "\n",
    "    df = pd.DataFrame(index=range(num_rows), columns=column_titles)\n",
    "\n",
    "    for ii, title in enumerate(column_titles):\n",
    "        cell_position = 0\n",
    "\n",
    "        for jj in range(num_rows):\n",
    "            for kk in range(num_cols):\n",
    "                if layout.iloc[jj, kk] == title:\n",
    "                    df.iloc[cell_position, ii] = data_plate_reader.iloc[jj, kk]\n",
    "                    cell_position += 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Function that enumerates through the groups of the layout\n",
    "##Extracts the data in the corresponding plate map\n",
    "##placing this information into the correct column\n",
    "def developing_new_df_from_multi_input_data(a_dictionary):\n",
    "    ##looping through how many files we have\n",
    "    for xx in range(len(a_dictionary)):\n",
    "\n",
    "        ##Creating the initial dataframe from the first data and layout\n",
    "        if xx==0:\n",
    "            #identifying the unique columns\n",
    "            column_titles = unique_samples_from_plate_layout(a_dictionary[xx][0])\n",
    "\n",
    "            ##determining the shape of this dataframe\n",
    "            num_rows = a_dictionary[xx][0].shape[0]\n",
    "            num_cols = a_dictionary[xx][0].shape[1]\n",
    "\n",
    "            ##creating an empty dataframe the size determined above\n",
    "            df = pd.DataFrame(index=range(num_rows), columns=column_titles)\n",
    "\n",
    "            ##iterating through each of the column names, extracting that data from the layout and the same position in the data, and placing this into the column with the respective name\n",
    "            for ii, title in enumerate(column_titles):\n",
    "                cell_position = 0\n",
    "\n",
    "                for jj in range(num_rows):\n",
    "                    for kk in range(num_cols):\n",
    "                        if a_dictionary[xx][0].iloc[jj, kk] == title:\n",
    "                            df.iloc[cell_position, ii] = a_dictionary[xx][1].iloc[jj, kk]\n",
    "                            cell_position += 1\n",
    "\n",
    "        \n",
    "        else:\n",
    "            ##identifing unique column names for the subsequent data layouts \n",
    "            new_column_titles = unique_samples_from_plate_layout(a_dictionary[xx][0])\n",
    "\n",
    "            ##creating a new dataframe containing the names of the columns that have been renamed\n",
    "            new_df = pd.DataFrame(index=range(num_rows), columns=new_column_titles)\n",
    "            \n",
    "            ##iterating through each of the column names, extracting that data from the layout and the same position in the data, and placing this into the column with the respective name\n",
    "            for ii, title in enumerate(new_column_titles):\n",
    "                cell_position = 0\n",
    "\n",
    "                for jj in range(num_rows):\n",
    "                    for kk in range(num_cols):\n",
    "                        if a_dictionary[xx][0].iloc[jj, kk] == title:\n",
    "                            new_df.iloc[cell_position, ii] = a_dictionary[xx][1].iloc[jj, kk]\n",
    "                            cell_position += 1\n",
    "\n",
    "            ##concatenating these newly made dataframes\n",
    "            df = pd.concat([df, new_df], axis=1)\n",
    "\n",
    "    # Find the column with the maximum non-NaN values\n",
    "    df_cleaned = df.dropna(how='all')\n",
    "\n",
    "    # Find duplicate column names\n",
    "    duplicate_columns = df_cleaned.columns[df_cleaned.columns.duplicated()]\n",
    "\n",
    "    # Iterate through duplicate columns\n",
    "    for column_name in duplicate_columns:\n",
    "        column_values = []  # List to store values from all columns with the same name\n",
    "        num_columns = df_cleaned[column_name].shape[1]  # Get the number of columns with the same name\n",
    "        \n",
    "        # Iterate through each column with the same name and extract values\n",
    "        for jj in range(num_columns):\n",
    "            column_values.append(df_cleaned[column_name].iloc[:, jj].tolist())  # Convert to list\n",
    "        \n",
    "        flattened_list = [item for sublist in column_values for item in sublist]\n",
    "        \n",
    "        # Ensure the flattened list length matches the length of the DataFrame index\n",
    "        if len(flattened_list) == len(df.index):\n",
    "            df_cleaned[column_name] = flattened_list\n",
    "        else:\n",
    "            # Insert NaN values to replace missing values of the dataframe\n",
    "            cleaned_list = [value for value in flattened_list if not pd.isnull(value)]\n",
    "\n",
    "            df1 = pd.DataFrame({column_name:cleaned_list})\n",
    "            df_cleaned = df_cleaned.drop(column_name, axis=1)\n",
    "\n",
    "            # Reindex the smaller dataframe with NaN values to match the larger dataframe's index\n",
    "            df_cleaned = df_cleaned.reindex(df1.index, fill_value=np.nan)\n",
    "\n",
    "            # Concatenate the dataframes along rows\n",
    "            df_cleaned = pd.concat([df1, df_cleaned], axis=1)\n",
    "            \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this will be a function where columns can be combined \n",
    "##this will help when multiple people are have varying naming convenitons\n",
    "def renaming_columns_of_a_dataframe(dataframe, names_that_are_the_same, names_they_should_be):\n",
    "    for ii in range(len(names_that_are_the_same)):\n",
    "        column_values = []  # List to store values from all columns with the same name\n",
    "        num_columns = dataframe[names_that_are_the_same[ii]].shape[1]  # Get the number of columns with the same name\n",
    "            \n",
    "        # Iterate through each column with the same name and extract values\n",
    "        for jj in range(num_columns):\n",
    "            column_values.append(dataframe[names_that_are_the_same[ii]].iloc[:, jj].tolist())  # Convert to list\n",
    "            \n",
    "        flattened_list = [item for sublist in column_values for item in sublist]\n",
    "        \n",
    "        if len(flattened_list) == len(dataframe.index):\n",
    "            dataframe = dataframe.drop(names_that_are_the_same[ii], axis=1)\n",
    "            dataframe[names_they_should_be[ii]] = flattened_list\n",
    "        elif len(flattened_list) > len(dataframe.index):\n",
    "             # Insert NaN values to replace missing values of the dataframe\n",
    "            cleaned_list = [value for value in flattened_list if not pd.isnull(value)]\n",
    "\n",
    "            df1 = pd.DataFrame({names_they_should_be[ii]:cleaned_list})\n",
    "            dataframe = dataframe.drop(names_that_are_the_same[ii], axis=1)\n",
    "            # Reindex the smaller dataframe with NaN values to match the larger dataframe's index\n",
    "            dataframe = dataframe.reindex(df1.index, fill_value=np.nan)\n",
    "             # Concatenate the dataframes along rows\n",
    "            dataframe = pd.concat([df1, dataframe], axis=1)\n",
    "\n",
    "        elif len(flattened_list) < len(dataframe.index):        \n",
    "            num_nan_values = len(dataframe.index) - len(flattened_list)\n",
    "            extended_list = flattened_list + [np.nan] * num_nan_values\n",
    "            if len(extended_list) != len(dataframe.index):\n",
    "                extended_list = extended_list[:len(dataframe.index)]  # Trim the list if it's still too long\n",
    "            dataframe[names_they_should_be[ii]] = extended_list\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = developing_new_df_from_multi_input_data(a_dict_of_all_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n",
    "print(df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_names = [\n",
    "    [\"blank (300ul)\", \"blank (200ul)\"],\n",
    "    [\"1:10 (300uL) T2\", \"1:10 (200uL) T2\", \"1:10 (200uL) T1\", \"1:10 (300uL) T1\"],\n",
    "    [\"1:1 (200uL) T1\", '1:1 (300uL) T2', \"1:1 (200uL) T2\", \"1:1 (300uL) T1\"],\n",
    "    [\"1:5 (200uL) T1\", \"1:5 (200uL) T2\", \"1:5 (300uL) T1\", \"1:5 (300uL) T1\", '1:5 (300uL) T2'],\n",
    "    ['DPBS (100uL)','DPBS (300uL)','DPBS (200uL)'],\n",
    "    ['3HC T1 (100uL)','3HC T1 (300uL)','3HC T1 (200uL)'],\n",
    "    ['Control T2 (300uL)','Control T2 (200uL)','Control T1 (300uL)','Control T2 (100uL)','Control T1 (100uL)','Control T1 (200uL)']\n",
    "]\n",
    "wanted_names = [\n",
    "    \"blank\", \n",
    "    \"1:10\",\n",
    "    \"1:1\",\n",
    "    \"1:5\",\n",
    "    'DPBS', \n",
    "    '3HC',\n",
    "    'Control'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.003, 0.0, 0.004, nan, -0.002, -0.002, -0.003, -0.003, nan]\n",
      "[0.081, 0.085, nan, nan, nan, nan, nan, nan, 0.048, 0.039, nan, nan, nan, nan, nan, nan, 0.051, 0.022, nan, nan, nan, nan, nan, nan, 0.075, 0.08, nan, nan, nan, nan, nan, nan]\n",
      "[0.398, 0.358, nan, nan, nan, nan, nan, nan, 0.654, 0.686, nan, nan, nan, nan, nan, nan, 0.46, 0.408, nan, nan, nan, nan, nan, nan, 0.623, 0.677, nan, nan, nan, nan, nan, nan]\n",
      "[0.098, 0.059, nan, nan, nan, nan, nan, nan, 0.105, 0.088, nan, nan, nan, nan, nan, nan, 0.15, 0.154, nan, nan, nan, nan, nan, nan, 0.15, 0.154, nan, nan, nan, nan, nan, nan, 0.159, 0.157, nan, nan, nan, nan, nan, nan]\n",
      "[0.03, 0.029, 0.034, nan, nan, nan, nan, nan, nan, nan, -0.021, -0.2, -0.022, nan, nan, nan, nan, nan, nan, nan, -0.01, -0.01, -0.009, nan, nan, nan, nan, nan, nan, nan]\n",
      "[1.654, nan, nan, nan, nan, nan, nan, nan, nan, 2.087, 1.757, 2.002, nan, nan, nan, nan, nan, nan, 1.953, 1.864, 1.982, nan, 2.02, nan, nan, nan, nan]\n",
      "[1.407, 1.396, 1.492, nan, nan, nan, nan, nan, 1.594, 1.593, 1.404, nan, nan, nan, nan, nan, 0.974, 0.93, 0.971, nan, nan, nan, nan, nan, 1.974, 1.885, 2.215, nan, nan, nan, nan, nan, 2.144, 2.99, 4.018, nan, nan, nan, nan, nan, 1.927, 2.801, 2.73, nan, nan, nan, nan, nan]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Control</th>\n",
       "      <th>3HC</th>\n",
       "      <th>DPBS</th>\n",
       "      <th>1:5</th>\n",
       "      <th>1:1</th>\n",
       "      <th>1:10</th>\n",
       "      <th>blank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.407</td>\n",
       "      <td>1.654</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.396</td>\n",
       "      <td>2.087</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.492</td>\n",
       "      <td>1.757</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.594</td>\n",
       "      <td>2.002</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.593</td>\n",
       "      <td>1.953</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.404</td>\n",
       "      <td>1.864</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.974</td>\n",
       "      <td>1.982</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.930</td>\n",
       "      <td>2.020</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.080</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.974</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.927</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.730</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Control    3HC   DPBS    1:5    1:1   1:10  blank\n",
       "0     1.407  1.654  0.030  0.098  0.398  0.081  0.001\n",
       "1     1.396  2.087  0.029  0.059  0.358  0.085  0.003\n",
       "2     1.492  1.757  0.034  0.105  0.654  0.048  0.000\n",
       "3     1.594  2.002 -0.021  0.088  0.686  0.039  0.004\n",
       "4     1.593  1.953 -0.200  0.150  0.460  0.051 -0.002\n",
       "5     1.404  1.864 -0.022  0.154  0.408  0.022 -0.002\n",
       "6     0.974  1.982 -0.010  0.150  0.623  0.075 -0.003\n",
       "7     0.930  2.020 -0.010  0.154  0.677  0.080 -0.003\n",
       "8     0.971    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "9     1.974    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "10    1.885    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "11    2.215    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "12    2.144    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "13    2.990    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "14    4.018    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "15    1.927    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "16    2.801    NaN    NaN    NaN    NaN    NaN    NaN\n",
       "17    2.730    NaN    NaN    NaN    NaN    NaN    NaN"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = renaming_columns_of_a_dataframe(df, similar_names, wanted_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_need_to_be_adjusted = [\n",
    "    '##change name to group name##', \n",
    "    '##change name to group name##'\n",
    "    ]\n",
    "diluted_from_original_density = [\n",
    "    '##change to correct dilution##', \n",
    "    '##change to correct dilution##'\n",
    "    ]\n",
    "final_volume_in_ml = ##change to correct final volume amount##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_the_original_density(df,columns_need_to_be_adjusted, diluted_from_original_density, final_volume_in_ml)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Graphing the data\n",
    "\n",
    "#set up the style, personally I am a fan of 'whitegrid'\n",
    "sns.set(style='##choose your style##')\n",
    "\n",
    "#setting up the figure, axes, and figure size\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6), dpi = 600)\n",
    "\n",
    "#Choosing the graphs and the x-axis groupins\n",
    "graph1 = ['##group 1##', '##group 2##', '##group 3##', '##group 4##',\n",
    "           '##group 5##', '##group 6##', '##group 7##', '##group 8##']\n",
    "graph2 = ['##group 1##', '##group 2##', '##group 3##', '##group 4##',\n",
    "           '##group 5##', '##group 6##', '##group 7##', '##group 8##']\n",
    "graph3 = ['##group 1##', '##group 2##', '##group 3##', '##group 4##',\n",
    "           '##group 5##', '##group 6##', '##group 7##', '##group 8##']\n",
    "\n",
    "#plotting the data\n",
    "sns.barplot(data=df[graph1], ax=axes[0])\n",
    "sns.lineplot(data=df[graph2], ax=axes[1])\n",
    "sns.boxplot(data=df[graph3], ax=axes[2])\n",
    "\n",
    "#choosing the labels and titles\n",
    "axes[0].set_xlabel('##set x label##')\n",
    "axes[0].set_ylabel('##set y label##')\n",
    "axes[0].set_title('## set title##')\n",
    "\n",
    "\n",
    "axes[1].set_xlabel('##set x label##')\n",
    "axes[1].set_ylabel('##set y label##')\n",
    "axes[1].set_title('## set title##')\n",
    "\n",
    "\n",
    "axes[2].set_xlabel('##set x label##')\n",
    "axes[2].set_ylabel('##set y label##')\n",
    "axes[2].set_title('## set title##')\n",
    "\n",
    "#figure supertitle\n",
    "fig.suptitle('##set figure super title##', fontsize=16)\n",
    "\n",
    "#ploting the graphs\n",
    "plt.tight_layout()\n",
    " \n",
    "##saving the data and graphs\n",
    "\n",
    "##plt.savefig('##please put the name that you want##.png' )\n",
    "\n",
    "#showing us the data\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = finding_statistically_significant_groups_update(##list of groups##, ##pvalue##, df, scaling_factor)\n",
    "axes.set_ylabel(##choose you y label)\n",
    "axes.set_title(##choose you title)\n",
    "\n",
    "##plt.savefig('##save file to name you want##.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
